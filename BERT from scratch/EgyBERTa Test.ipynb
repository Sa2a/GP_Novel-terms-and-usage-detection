{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\river\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\river\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\river\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Users\\river\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = pipeline('fill-mask', model='EgyBERTa', tokenizer='EgyBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.016952363774180412,\n",
       "  'token': 302,\n",
       "  'token_str': ' من',\n",
       "  'sequence': 'كيف نترك من بلا تخفيف'},\n",
       " {'score': 0.014255023561418056,\n",
       "  'token': 225,\n",
       "  'token_str': ' ',\n",
       "  'sequence': 'كيف نترك  بلا تخفيف'},\n",
       " {'score': 0.013390934094786644,\n",
       "  'token': 275,\n",
       "  'token_str': ' و',\n",
       "  'sequence': 'كيف نترك و بلا تخفيف'},\n",
       " {'score': 0.008837600238621235,\n",
       "  'token': 320,\n",
       "  'token_str': ' ان',\n",
       "  'sequence': 'كيف نترك ان بلا تخفيف'},\n",
       " {'score': 0.008807444013655186,\n",
       "  'token': 333,\n",
       "  'token_str': ' فى',\n",
       "  'sequence': 'كيف نترك فى بلا تخفيف'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# دا مجرد رأى و اله اعلم و ابقا\n",
    "# masking word (اله)\n",
    "fill(\"كيف نترك <mask> بلا تخفيف\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get contextual word embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, AutoModel,RobertaForMaskedLM,AutoTokenizer\n",
    "\n",
    "\n",
    "def get_word_idx(sent: str, word: str):\n",
    "    return sent.split(\" \").index(word)\n",
    "\n",
    "\n",
    "def get_hidden_states(encoded, token_ids_word, model, layers):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "    Select only those subword token outputs that belong to our word of interest\n",
    "    and average them.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    "\n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "    # Only select the tokens that constitute the requested word\n",
    "    word_tokens_output = output[token_ids_word]\n",
    "\n",
    "    return word_tokens_output.mean(dim=0)\n",
    "\n",
    "\n",
    "def get_word_vector(sent, idx, tokenizer, model, layers):\n",
    "    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "    that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "    # get all token idxs that belong to the word of interest\n",
    "    token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "\n",
    "    return get_hidden_states(encoded, token_ids_word, model, layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test word similarity with cosine similarity sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EgyBERTa were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at EgyBERTa and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0237e-01, -1.9478e+00, -3.0412e+00, -3.7727e+00, -3.3422e+00,\n",
       "        -6.5008e+00,  1.1251e+00, -9.0984e-01, -4.3397e+00, -6.8665e+00,\n",
       "        -1.6424e+00, -4.9605e+00, -5.0162e+00, -1.7020e-01, -5.9561e+00,\n",
       "        -1.4974e+00, -4.5487e+00, -4.7444e-01,  2.4535e+00,  9.6574e-01,\n",
       "        -1.2342e+00,  6.5007e+00, -2.0057e+00,  1.6802e+00, -3.4554e+00,\n",
       "         3.4686e+00, -2.8246e+00,  6.7783e+00,  1.4221e+00, -1.7852e+00,\n",
       "         1.0039e+01,  4.6123e+00, -3.9829e+00, -3.0084e+00, -1.0839e+00,\n",
       "         3.7969e+00, -2.8679e+00,  6.8511e+00, -6.1027e+00, -1.7925e+00,\n",
       "        -5.8851e-01, -3.5449e+00, -2.0798e-01,  6.2225e+00, -1.4809e+00,\n",
       "         4.7526e-02,  4.1646e+00, -3.8881e+00,  2.6679e+00,  4.3125e+00,\n",
       "        -7.2887e+00, -1.9245e+00,  8.2278e-01, -2.3780e+00,  4.0661e+00,\n",
       "        -4.0736e+00, -5.1413e-01, -2.7918e+00, -6.5185e+00, -2.9290e+00,\n",
       "         1.0714e+00,  6.2348e+00,  2.4318e+00,  7.6076e-01, -4.1703e+00,\n",
       "        -3.4822e+00, -8.1879e+00, -3.3057e+00,  1.1053e+00, -1.5830e+00,\n",
       "        -1.1396e+00, -9.8059e-01, -2.0281e+00,  6.6569e+00, -1.4537e+00,\n",
       "        -6.6777e-01, -5.4064e+00,  2.3194e+00,  1.6857e+00, -3.9770e+00,\n",
       "        -7.7553e-01, -2.6519e+00, -1.3855e+00, -9.1731e-01, -3.7477e-01,\n",
       "         8.3180e+00,  1.3535e+00, -1.6771e-01, -1.2712e+00,  4.1391e+00,\n",
       "        -2.9175e+00, -2.2687e+00,  6.4295e-01, -4.3713e+00,  4.6449e+00,\n",
       "        -3.5687e+00, -3.5008e+00,  3.1960e-01, -5.9619e+00, -6.4137e+00,\n",
       "         1.5181e+00, -3.4568e+00,  8.2860e-01, -4.5347e+00, -1.1918e+00,\n",
       "        -4.5547e+00,  5.3018e+00, -4.0671e+00,  5.7140e+00,  2.0636e+00,\n",
       "        -2.5972e+00, -2.8905e+00, -4.8332e-02, -7.4475e+00, -3.2445e+00,\n",
       "         3.3181e-01, -8.0441e-02, -1.9814e+00, -2.4209e+00, -3.1269e-02,\n",
       "         2.0794e+00, -1.6714e+00,  5.1829e+00,  2.6659e+00, -3.2003e+00,\n",
       "         4.2326e+00, -2.5704e+00, -1.5608e+00,  2.7465e+00, -1.4580e-01,\n",
       "        -3.5360e+00, -6.6044e+00, -5.0187e+00,  1.5234e+00, -2.8447e+00,\n",
       "        -5.7048e+00, -1.6819e+00, -3.2713e+00, -2.1716e+00,  9.0657e-01,\n",
       "        -7.2813e-01,  1.4543e-01, -1.5140e+00,  5.6317e+00, -1.1310e+00,\n",
       "        -3.2299e+00, -1.0540e+00,  1.5325e+00,  2.7046e+00, -8.5754e+00,\n",
       "         4.7502e+00,  2.7153e+00, -7.4118e+00, -5.8602e+00,  2.6721e+00,\n",
       "         7.4178e+00, -1.1232e+00, -8.1592e+00, -5.7091e-02, -3.8120e+00,\n",
       "         5.8028e+00,  5.3967e+00, -6.2468e+00, -3.2033e+00, -6.1685e-01,\n",
       "        -3.9015e+00, -6.1383e+00,  1.7999e-02, -5.3114e+00, -7.7087e-01,\n",
       "        -1.5301e+00, -9.3793e-01,  1.7271e+00, -4.0049e+00, -3.1797e+00,\n",
       "         2.0631e+00, -7.6254e+00, -4.0918e-01, -2.4891e+00,  1.3418e+00,\n",
       "         2.0408e+00,  1.0994e-01, -6.6686e-01,  3.8996e-01,  4.0756e+00,\n",
       "         2.4156e+00,  8.0745e-01,  5.6242e+00,  6.1942e-01, -1.6212e+00,\n",
       "        -1.4595e+00,  2.6008e+00,  6.0225e+00, -2.3580e-01, -5.0219e+00,\n",
       "         5.6267e+00,  2.3538e+00,  2.0261e+00, -1.1790e+00,  4.7445e+00,\n",
       "         5.6424e+00,  2.0266e+00,  3.5944e+00, -9.5276e+00, -5.4522e+00,\n",
       "         3.0288e-01,  7.8918e+00, -1.2814e+00, -2.0065e+00, -3.6364e+00,\n",
       "         2.9231e+00, -1.5407e+00,  1.3562e+00,  5.1774e+00,  6.8258e+00,\n",
       "        -3.2395e+00,  3.3264e+00, -6.6643e+00, -6.7474e-01,  2.0515e+00,\n",
       "         1.4476e+00,  4.4066e+00, -5.9019e-01,  9.0866e+00, -1.9325e+00,\n",
       "        -1.5022e+01,  1.4769e-01,  3.4054e+00,  9.3064e+00,  6.1975e+00,\n",
       "         1.9096e-01, -4.6785e+00, -1.7995e+00,  5.0229e-01,  4.1985e+00,\n",
       "         1.4241e+00,  3.2319e+00, -2.2525e+00,  1.2795e+00,  7.3275e-01,\n",
       "         2.4129e+00, -1.2426e+00, -5.9337e+00, -1.3060e+00,  2.0470e+00,\n",
       "         1.8765e+00,  5.7335e+00,  3.6434e+00,  9.3618e+00,  3.7630e+00,\n",
       "        -8.5247e+00, -8.0408e-01, -2.6237e+00,  8.1562e-01, -2.9290e+00,\n",
       "         7.6343e+00, -3.7420e-02, -2.9184e+00,  6.1024e+00,  3.7581e+00,\n",
       "         5.5868e+00, -6.9046e+00, -7.6799e+00, -1.8261e+00, -3.9072e+00,\n",
       "        -1.0292e-01,  1.1010e+01, -2.6435e+00, -1.7224e+00, -4.7618e+00,\n",
       "        -4.6642e+00,  1.6504e+00, -1.9230e-02,  1.4901e+00,  4.7303e-01,\n",
       "         5.4862e+00, -9.1094e+00,  9.8886e+00,  3.2290e+00, -7.2553e+00,\n",
       "        -2.0885e+00, -1.8141e+00,  3.5197e+00,  8.0411e+00,  3.6264e+00,\n",
       "         2.1485e+00, -2.0134e+00, -9.5213e-01, -8.8985e-01, -2.7073e+00,\n",
       "        -4.6280e-02,  3.6538e-01, -3.4453e+00,  3.3299e+00,  5.4994e+00,\n",
       "         5.3938e+00, -3.6033e+00, -3.2926e+00,  1.6405e+00,  4.4180e-01,\n",
       "        -4.8728e+00,  3.8210e+00,  7.7106e-01,  1.2221e+00, -2.3945e-01,\n",
       "        -1.3824e+00,  3.3377e+00,  2.9282e+00, -3.9564e-01, -1.6722e+00,\n",
       "         2.6499e+00, -3.9999e+00,  7.7406e-01, -5.5655e+00,  1.2665e+00,\n",
       "        -2.3578e+00, -6.7279e-01,  2.3703e-01,  2.2576e+00, -4.1961e+00,\n",
       "         6.9175e-01,  2.2339e+00, -2.2601e+00,  2.3909e+00, -3.2424e+00,\n",
       "         2.9189e+00,  1.4275e+00, -2.1945e+00,  1.7915e+00,  3.9085e+00,\n",
       "        -9.3966e+00, -8.0781e-01, -1.3197e-01,  4.0292e+00, -1.0127e+00,\n",
       "        -1.2842e+00, -1.4667e+00,  6.1324e-01,  4.6311e+00,  2.9008e+00,\n",
       "        -1.2731e+00, -5.3754e+00,  6.1832e+00, -6.9518e-02, -1.9342e+00,\n",
       "        -6.0914e+00,  1.1493e+00, -6.2120e+00,  5.7228e+00,  3.2370e-01,\n",
       "        -4.0427e-01,  4.2288e+00, -3.4822e+00, -1.6960e+00, -4.7607e+00,\n",
       "         1.3514e+00, -5.6756e-01, -1.4924e+00, -1.0419e+00, -1.6700e+00,\n",
       "        -3.8162e+00, -2.9088e+00,  5.7765e+00,  3.1608e+00,  2.3874e+00,\n",
       "        -1.8800e+00,  2.5478e+00,  1.3684e+00, -4.0720e+00,  2.4532e+00,\n",
       "        -5.3887e-01, -5.9310e-01, -2.9418e+00, -2.2418e+00, -5.4358e+00,\n",
       "        -5.4593e+00, -8.2584e+00, -4.0942e+00, -3.0149e+00,  1.5561e+00,\n",
       "         7.4154e+00,  4.3892e-01,  5.1711e+00, -2.2703e+00, -4.1960e+00,\n",
       "        -4.3668e+00, -2.2237e+00,  7.2655e+00, -7.9402e+00,  4.3659e+00,\n",
       "         7.6564e-01,  3.3181e+00,  7.9672e+00,  1.4057e+00,  3.6940e+00,\n",
       "        -5.1487e+00,  3.1819e-01,  5.9190e-01, -6.1348e+00, -6.0195e+00,\n",
       "        -3.6170e+00, -3.7787e+00,  1.0177e+00,  1.7551e+00, -6.0085e+00,\n",
       "        -5.4115e-01,  2.2568e+00, -2.6744e-01,  1.3142e+00,  3.8018e+00,\n",
       "         2.0549e+00,  4.2921e+00,  4.1203e+00,  1.9065e+00,  2.8045e+00,\n",
       "         8.1494e-01,  3.0632e+00,  1.4795e+00,  2.0572e+00, -1.1092e+00,\n",
       "        -3.4039e-01, -1.6393e+00, -9.1959e-02,  3.3548e+00,  4.0273e-01,\n",
       "        -1.3145e+00,  4.6445e+00,  1.4310e-01,  5.0573e+00, -1.0317e+00,\n",
       "         2.9901e+00,  3.5344e+00, -3.4199e+00,  2.6713e+00, -1.3189e-01,\n",
       "        -6.4311e+00, -3.0030e+00, -2.6398e+00, -9.1463e-01, -1.6625e+00,\n",
       "        -4.3395e-02,  1.0451e+00, -6.4354e-01,  2.3802e-01,  1.2423e+00,\n",
       "         2.7153e+00, -3.7471e+00,  1.1943e-01, -2.0141e+00,  3.0500e+00,\n",
       "        -5.1980e-01,  6.3541e-01,  3.0641e+00, -1.8525e-01,  9.2186e+00,\n",
       "         5.5556e+00,  5.6420e+00,  7.6953e+00,  3.0913e+00,  4.4778e+00,\n",
       "         2.8586e+00, -1.8768e+00, -3.8200e-01, -2.8979e-01,  3.9285e+00,\n",
       "         2.0673e+00,  4.2300e+00,  5.2639e+00,  4.0016e+00, -2.8509e+00,\n",
       "         7.3492e+00,  1.3097e+00,  1.5423e+00,  4.9127e+00,  1.3636e+00,\n",
       "        -1.5045e-01, -3.3363e+00, -1.3090e+00,  9.3683e-02, -1.1662e+01,\n",
       "         1.4280e+00,  1.9236e+00, -3.9257e+00, -3.0886e+00,  1.8692e+00,\n",
       "         3.7492e+00,  4.5986e+00, -4.6034e+00,  2.1426e+00,  3.0509e+00,\n",
       "         3.2450e+00, -3.6786e+00,  5.6391e+00,  5.8198e+00,  2.6117e-03,\n",
       "         1.4764e+00,  5.3605e+00, -4.3128e+00,  2.8833e+00, -2.0487e+00,\n",
       "         5.0987e+00,  1.2386e+00,  4.1681e-01,  1.3476e+00, -9.6935e-01,\n",
       "         7.6400e+00,  3.5754e+00, -3.8188e+00,  1.4354e-01, -6.1830e+00,\n",
       "        -1.9917e+00,  1.8621e+00,  2.6616e+00,  2.0331e+00,  6.7924e+00,\n",
       "        -3.6887e+00,  5.2648e+00, -1.8814e+00,  6.1681e+00, -7.2096e-01,\n",
       "         4.2698e-01, -3.3284e+00,  1.8792e+00, -5.5221e+00,  1.5754e+00,\n",
       "         1.3982e+00,  8.3750e-01,  3.8222e+00, -1.3066e+00,  5.0014e+00,\n",
       "         3.2465e+00,  6.5723e+00,  3.1497e-01, -1.4705e+00, -3.0735e+00,\n",
       "         6.5404e+00, -2.0278e+00,  3.8614e+00, -2.7893e+00,  3.8583e+00,\n",
       "         6.8646e+00, -1.4769e-03,  1.1425e+00, -6.3438e-01,  1.7877e+00,\n",
       "         2.4684e-01, -3.9458e+00, -2.4162e+00,  4.7257e+00,  3.2142e+00,\n",
       "         1.0196e+00,  1.5155e+00,  5.9179e+00, -7.1696e+00, -2.1328e+00,\n",
       "        -2.2270e+00,  3.1255e-01, -1.4545e-01,  2.4295e+00, -2.9866e+00,\n",
       "        -7.3933e+00,  2.2854e-01,  4.4776e+00, -9.6640e+00,  9.2610e-01,\n",
       "        -3.5792e+00,  3.6959e-01,  1.7340e+00, -3.8829e-01,  3.9983e+00,\n",
       "         8.4316e+00,  8.1443e+00,  9.3126e+00,  2.8794e+00, -1.3170e+00,\n",
       "        -4.2251e+00, -5.5145e+00, -1.7676e+00, -3.1413e+00,  2.6836e+00,\n",
       "         1.4515e+00, -2.5894e+00,  3.4065e+00,  2.1857e+00, -3.1152e+00,\n",
       "        -3.9545e-01, -2.6932e-01,  1.7199e+00,  2.6012e+00, -6.2291e+00,\n",
       "        -7.8347e-01, -2.9516e+00,  7.5434e-01, -3.4251e+00, -3.1809e+00,\n",
       "         1.2409e+00, -4.4856e+00, -1.7804e-01, -4.4512e+00,  2.5124e+00,\n",
       "        -1.7837e+00,  3.0036e+00, -2.2696e+00, -8.4697e-01,  2.0624e+00,\n",
       "         2.6785e+00,  2.2500e+00, -8.2437e+00,  2.8422e+00,  1.8433e+00,\n",
       "        -1.8950e+00,  2.8373e+00, -2.5597e+00,  1.8524e+00,  3.7265e+00,\n",
       "        -3.3109e+00, -1.6514e+00,  1.4408e+00,  7.5576e-01,  2.8644e-01,\n",
       "         6.6061e+00,  1.3272e+00, -1.5084e+00,  2.5181e+00,  5.0122e-01,\n",
       "        -2.5507e+00, -2.2875e+00, -1.4567e+00,  9.3464e+00,  4.1354e+00,\n",
       "        -7.0628e+00, -5.6638e+00, -9.8003e+00,  2.0336e+00,  5.2545e+00,\n",
       "        -7.2902e-01, -2.8785e+00, -2.7206e+00,  2.5778e+00, -3.1393e+00,\n",
       "         6.8646e+00, -3.4420e-01,  2.3497e+00, -1.7834e-01, -7.4844e+00,\n",
       "        -7.1490e+00, -7.9371e-01,  1.3661e+00,  1.1009e+00,  4.9488e-01,\n",
       "         4.8761e+00,  9.2907e+00, -6.2332e+00, -1.7535e+00,  9.4285e-01,\n",
       "        -3.2879e-01, -8.6511e-01, -4.5390e+00,  8.2906e-01,  3.3244e+00,\n",
       "         1.3894e+00,  5.7004e+00, -5.0235e-01, -3.0236e+00, -4.3248e+00,\n",
       "         8.2172e-01, -5.1967e+00,  2.1840e-01, -2.0449e-01, -1.7755e+00,\n",
       "        -7.6765e+00, -4.2346e+00,  2.0885e+00,  5.9354e-02, -5.0953e+00,\n",
       "        -1.2429e+00,  5.6040e+00, -4.3904e+00,  8.3538e+00, -4.9610e-01,\n",
       "        -7.4879e-01,  2.4712e+00, -3.8211e+00,  1.5393e+00,  3.9769e+00,\n",
       "        -2.5014e+00,  2.2145e+00,  8.6694e-01,  3.1956e+00, -3.7833e+00,\n",
       "        -4.2498e+00,  5.5268e-01, -2.0294e+00, -6.3758e+00, -1.7573e+00,\n",
       "        -3.8771e-02, -1.2939e-01, -7.1950e+00,  2.5515e+00, -1.7008e+00,\n",
       "        -3.1579e-02,  4.2733e+00,  2.7069e+00, -8.8592e-01,  6.0946e+00,\n",
       "        -3.7695e+00,  1.3737e-01,  1.8173e+00, -1.9327e+00,  5.3837e+00,\n",
       "         2.5239e+00,  5.4597e+00, -4.3926e+00, -8.9210e-02,  5.3612e+00,\n",
       "        -2.2847e+00,  3.6280e+00, -2.9722e+00,  4.6317e+00,  1.5757e+00,\n",
       "        -3.7610e+00,  1.2458e+01, -2.1561e+00,  3.7499e-01, -6.0208e+00,\n",
       "         1.9652e+00,  2.0527e+00,  7.7746e-01,  1.2775e+00,  4.2733e+00,\n",
       "        -6.3710e+00,  7.7614e+00, -6.6990e+00,  7.3581e-01, -2.1799e-01,\n",
       "         2.0542e+00, -1.0429e+00, -4.4035e+00, -3.6657e+00, -1.2269e+00,\n",
       "         7.0930e-01,  4.3492e-01,  9.8300e-01, -2.4509e+00,  4.1961e-01,\n",
       "         4.3911e+00,  1.8014e+00, -7.5428e+00, -1.6323e+00, -2.9219e+00,\n",
       "        -4.4230e+00, -1.3287e+00, -2.3698e+00,  5.5572e+00, -3.1276e-01,\n",
       "        -8.7129e-01,  2.6663e+00,  1.8887e+00,  5.5908e+00, -1.5755e+00,\n",
       "        -4.7026e-01,  7.8287e+00, -1.2307e+00, -3.8727e+00, -5.5003e+00,\n",
       "        -4.0460e+00,  5.0589e+00,  2.1830e+00])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use last four layers by default\n",
    "\n",
    "layers = [-4, -3, -2, -1] \n",
    "tokenizer = AutoTokenizer.from_pretrained('EgyBERTa', max_len=512)\n",
    "model = AutoModel.from_pretrained(\"EgyBERTa\", output_hidden_states=True)\n",
    "\n",
    "sent = \"الحباه حلوه\" \n",
    "idx = get_word_idx(sent, \"حلوه\")\n",
    "\n",
    "word_embedding = get_word_vector(sent, idx, tokenizer, model, layers)\n",
    "word_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = \"الحباه معفنه\" \n",
    "idx2 = get_word_idx(sent2, \"معفنه\")\n",
    "\n",
    "word_embedding2 = get_word_vector(sent2, idx2, tokenizer, model, layers)\n",
    "\n",
    "sent3 = \"الحباه جميله\" \n",
    "idx3 = get_word_idx(sent3, \"جميله\")\n",
    "\n",
    "word_embedding3 = get_word_vector(sent3, idx3, tokenizer, model, layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding = np.array(word_embedding).reshape(1,-1)\n",
    "word_embedding2 = np.array(word_embedding2).reshape(1,-1)\n",
    "word_embedding3 = np.array(word_embedding3).reshape(1,-1)\n",
    "word_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* حلوه vs معفنه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55054104]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(word_embedding,word_embedding2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* حلوه vs جميله"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46786463]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(word_embedding,word_embedding3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* جميله vs معفنه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5592834]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(word_embedding2,word_embedding3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa01416571b4275a2309b961d1bd16d947c31c3c9a0101a66fce0e66d3b1e2ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
