{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5vi4bPHx_mm"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()"
      ],
      "metadata": {
        "id": "de1kSRqHyJ8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(files=paths[:5], vocab_size=30_522, min_frequency=2,\n",
        "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
      ],
      "metadata": {
        "id": "MkMJ7WPayL5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.mkdir('./egyberto')\n",
        "\n",
        "tokenizer.save_model('egyberto')"
      ],
      "metadata": {
        "id": "qCcqZpFXyZuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "id": "I1DaB__gyrEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "# initialize the tokenizer using the tokenizer we initialized and saved to file\n",
        "tokenizer = RobertaTokenizer.from_pretrained('egyberto', max_len=512)"
      ],
      "metadata": {
        "id": "8-Hz3r1cyxwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token=[]\n",
        "for i in range(1,10):\n",
        "  tokens = tokenizer(data['Text'][i])\n",
        "  token.append(tokens) "
      ],
      "metadata": {
        "id": "adW6ebzoy1ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token"
      ],
      "metadata": {
        "id": "lxk_dRDwzQFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(data['Text'][0])\n",
        "tokens"
      ],
      "metadata": {
        "id": "ekxdzBRuzSyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer('مرحبا سارة')"
      ],
      "metadata": {
        "id": "vuIVA48TzWea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "id": "d0l29IJ1ycFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}