{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5vi4bPHx_mm"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "de1kSRqHyJ8a"
      },
      "outputs": [],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "paths = [str(x) for x in Path(\"../Data/Training ready/RestOf_AOC_youm7_comments/\").glob('**/*.txt')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MkMJ7WPayL5z"
      },
      "outputs": [],
      "source": [
        "tokenizer.train(files=paths, vocab_size=30_522, min_frequency=2,\n",
        "                special_tokens=['<pad>','<s>',  '</s>', '<unk>', '<mask>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qCcqZpFXyZuy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['EgyBERTa\\\\vocab.json', 'EgyBERTa\\\\merges.txt']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.mkdir('./EgyBERTa')\n",
        "\n",
        "tokenizer.save_model('EgyBERTa')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1DaB__gyrEC"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8-Hz3r1cyxwa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\river\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "c:\\Users\\river\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
            "c:\\Users\\river\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "# initialize the tokenizer using the tokenizer we initialized and saved to file\n",
        "tokenizer = RobertaTokenizer.from_pretrained('EgyBERTa', max_len=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vuIVA48TzWea"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer('مرحبا سارة')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d0l29IJ1ycFq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [1, 13283, 14966, 2], 'attention_mask': [1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['_MutableMapping__marker',\n",
              " '__abstractmethods__',\n",
              " '__class__',\n",
              " '__contains__',\n",
              " '__copy__',\n",
              " '__delattr__',\n",
              " '__delitem__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__reversed__',\n",
              " '__setattr__',\n",
              " '__setitem__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_abc_impl',\n",
              " '_encodings',\n",
              " '_n_sequences',\n",
              " 'char_to_token',\n",
              " 'char_to_word',\n",
              " 'clear',\n",
              " 'convert_to_tensors',\n",
              " 'copy',\n",
              " 'data',\n",
              " 'encodings',\n",
              " 'fromkeys',\n",
              " 'get',\n",
              " 'is_fast',\n",
              " 'items',\n",
              " 'keys',\n",
              " 'n_sequences',\n",
              " 'pop',\n",
              " 'popitem',\n",
              " 'sequence_ids',\n",
              " 'setdefault',\n",
              " 'to',\n",
              " 'token_to_chars',\n",
              " 'token_to_sequence',\n",
              " 'token_to_word',\n",
              " 'tokens',\n",
              " 'update',\n",
              " 'values',\n",
              " 'word_ids',\n",
              " 'word_to_chars',\n",
              " 'word_to_tokens',\n",
              " 'words']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dir(tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "fa01416571b4275a2309b961d1bd16d947c31c3c9a0101a66fce0e66d3b1e2ad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
