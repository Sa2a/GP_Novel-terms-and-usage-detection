{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\river\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at ../MARBERT_pytorch_verison/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../MARBERT_pytorch_verison/\",local_files_only=True)\n",
    "model = BertForMaskedLM.from_pretrained(\"../MARBERT_pytorch_verison/\",local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "fill = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoss(true_sentence ,masked_sentence):\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # retrieve index of [MASK]\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "    # print(logits.shape)\n",
    "    predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "    # print(predicted_token_id)\n",
    "    labels = tokenizer(true_sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    # mask labels of non-[MASK] tokens\n",
    "    labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    return (tokenizer.decode(predicted_token_id), round(outputs.loss.item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def getTopPred(masked_sentence,topN = 100):\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "    predicted_token_id = logits[0, mask_token_index]    \n",
    "    topN_tokend_id = np.argpartition(predicted_token_id.reshape(-1), -topN)[-topN:]\n",
    "    return tokenizer.decode(topN_tokend_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'الناس❉ لن اقدرالله ربما الدين هنا دا بعدين ربي لكى هى فيما النتيجة والله البادي ربى نا انتا المولى فعلا سبحانه لم بكره الذى يارب الباقى هو الواقع بالله ده هم انتم فالله اكيد رب كان تعالى ربنا بس وحده ربكم الرب الرحمن رينا اللى وربنا ريي انا انت الشارع الحق اللہ ربناا لكن اكاد اللهم ما لله هوا انى نعم سوف الرسول الموفق لا الشرع احنا ان الله بكرة غدا ﷲ اللھ url اللهه هوه ربك القادم االله فقط نحن المستقبل واللة كنت الحاضر هما العباد رسوله الخالق حضرتك الل التاريخ عايز م الفعل طبعا انتى اللة'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTopPred(\"دا مجرد رأى و[MASK] اعلم و ابقا\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 100000])\n",
      "tensor([1944])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('الله', 0.11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLoss(true_sentence=\"دا مجرد رأى و الله اعلم و ابقا\", masked_sentence=\"دا مجرد رأى و[MASK] اعلم و ابقا\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8920164108276367,\n",
       "  'token': 1944,\n",
       "  'token_str': 'ا ل ل ه',\n",
       "  'sequence': 'دا مجرد راى و الله اعلم و ابقا'},\n",
       " {'score': 0.05682622268795967,\n",
       "  'token': 2410,\n",
       "  'token_str': 'ر ب ن ا',\n",
       "  'sequence': 'دا مجرد راى و ربنا اعلم و ابقا'},\n",
       " {'score': 0.012048384174704552,\n",
       "  'token': 2043,\n",
       "  'token_str': 'ا ن ا',\n",
       "  'sequence': 'دا مجرد راى و انا اعلم و ابقا'},\n",
       " {'score': 0.008924596942961216,\n",
       "  'token': 2188,\n",
       "  'token_str': 'و ا ل ل ه',\n",
       "  'sequence': 'دا مجرد راى و والله اعلم و ابقا'},\n",
       " {'score': 0.0060321856290102005,\n",
       "  'token': 16954,\n",
       "  'token_str': 'ر ب ى',\n",
       "  'sequence': 'دا مجرد راى و ربى اعلم و ابقا'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# دا مجرد رأى و الله اعلم و ابقا\n",
    "fill(\"دا مجرد رأى و[MASK] اعلم و ابقا\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('مسافر', 6.15)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLoss(true_sentence=\"هو كل يوم نسمع فتوى جديده  ماكلنا عارفين من كان مريضا أو على سفر وخلصت  هنخترع بقى رخص جديده من عندنا \", masked_sentence=\"هو كل يوم نسمع فتوى جديده  ماكلنا عارفين من كان [MASK] أو على سفر وخلصت  هنخترع بقى رخص جديده من عندنا \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"المشتروع\" in tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07754268497228622,\n",
       "  'token': 12264,\n",
       "  'token_str': 'ع ج ل',\n",
       "  'sequence': 'انا رايح اركب عجل'},\n",
       " {'score': 0.052763354033231735,\n",
       "  'token': 33856,\n",
       "  'token_str': 'ت ا ك س ي',\n",
       "  'sequence': 'انا رايح اركب تاكسي'},\n",
       " {'score': 0.04821640998125076,\n",
       "  'token': 30165,\n",
       "  'token_str': 'ا ل ق ط ر',\n",
       "  'sequence': 'انا رايح اركب القطر'},\n",
       " {'score': 0.04384848475456238,\n",
       "  'token': 60969,\n",
       "  'token_str': 'ت و ك ت و ك',\n",
       "  'sequence': 'انا رايح اركب توكتوك'},\n",
       " {'score': 0.04264984279870987,\n",
       "  'token': 1936,\n",
       "  'token_str': 'u r l',\n",
       "  'sequence': 'انا رايح اركب url'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# انا رايح اركب المشتروع\n",
    "fill(\"انا رايح اركب [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'اوتوستراد الهرم تاني قارب البوكيمون جنازة مواصلة فيراري موتو صاروخ واحد السيسي كفر دلوقتي القمر. السيارة الماتش المشروع اليكس التوكتوك مراجيح ترام حديد طيارات البحر ورا الصحراوي السياره بنزين الموج تقويم عربية عربيه بوكيمون الاسعاف سواق الزمالك مركب حافلة الجيش لوحدي مواصله ايه سفينة الشنطه الدايري [UNK] حشيش مكنة المحور تكسي بترول bmw الخيل الحصان حصان المرسيدس اهو الصاروخ المواصلات الهوا حمار العجله الميكروباص اتوبيس عجلة خيل ترحال مترو العربية طياره مرسيدس الموجه عجل مواصلات الطياره url القطر كريم الموجة العجلة العربيه طيارة قطار قطر الطيارة توكتوك اوبر موجة ميكروباص المترو الباص تاكسي باص جمل الاتوبيس القطار التاكسي عجله'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_top_100 = getTopPred(\"انا رايح اركب [MASK]\")\n",
    "pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"المشروع\" in tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"المشروع\" in pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"المكتب\" in pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('عجل', 6.78)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_actual) = getLoss(true_sentence=\"انا رايح اركب المشروع\", masked_sentence=\"انا رايح اركب [MASK]\")\n",
    "(predicted, loss_actual) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('عجل', 2.56)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_relevant) = getLoss(true_sentence=\"انا رايح اركب عجل\", masked_sentence=\"انا رايح اركب [MASK]\")\n",
    "(predicted, loss_relevant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('عجل', 10.85)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_not_relevant) = getLoss(true_sentence=\"انا رايح اركب المكتب\", masked_sentence=\"انا رايح اركب [MASK]\")\n",
    "(predicted, loss_not_relevant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Relevance 92.89 %\n",
      "Percentage of Not Relevant 7.11 %\n"
     ]
    }
   ],
   "source": [
    "# add award point to relevant distance because it appeared in the top 100 predicted words \n",
    "point = .5 * loss_actual \n",
    "\n",
    "loss_actual = loss_actual- point\n",
    "loss_not_relevant = loss_not_relevant + point\n",
    "\n",
    "actual_relevant_dist = abs( loss_actual - loss_relevant  )\n",
    "actual_not_relevant_dist = abs( loss_actual - loss_not_relevant )\n",
    "\n",
    "total_distance  = actual_relevant_dist + actual_not_relevant_dist\n",
    "\n",
    "actual_relevant_precent = actual_not_relevant_dist  / total_distance\n",
    "actual_notrelevant_precent = actual_relevant_dist / total_distance\n",
    "\n",
    "print(f\"Percentage of Relevance {round(actual_relevant_precent * 100 , 2)} %\" )\n",
    "print(f\"Percentage of Not Relevant {round(actual_notrelevant_precent * 100 , 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_relevant_precent + actual_notrelevant_precent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After correcting \"نرم\" >> \"نجم\"\n",
    "true_sentence = \"بقينا ف زمن اللي سب الدين مش ف لسانه ده نجم\"\n",
    "masked_sentence = \"بقينا ف زمن اللي سب الدين مش ف لسانه ده [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"نرم\" in tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"نجم\" in tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"نجم\" in pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"😏😏 دلوقتي ارحمونا تاني♨ 💔 اخلاقنا 😑😑 - ارهاب دينه 👌🏻 ابتلاء ☺ تويتر غلط اخلاق جهل 😅 😡 😓 😀 خلاص 😞 عادي مجتمع🌟 النفاق 😠 😊☁ مصيبه كمان انتشر اسلوب كله ستايل ✋✋ قرف. موضه اخره الواقع الطبيعي مرض حقيقه بس اسمه والله لعنه6 👎 ✋ 😏 👌 هبل 👊 خالص 😐 نفاق للاسف بقي هزار 😑 حوار كلام 😃 بجد حقيقي 😂 تخلف ؟ الزمن 😒 بقى٨ عيب واقع'حلال كافر 😔 فرعون حقيقة 😕 😂😂 حالنا مجتمعنا [UNK] ، url! حرام حاله زمان زمن بقا كفر ✋🏻 😒😒\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_top_100 = getTopPred(masked_sentence)\n",
    "pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"المكتب\" in pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1302552968263626,\n",
       "  'token': 16,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'بقينا ف زمن اللي سب الدين مش ف لسانه ده.'},\n",
       " {'score': 0.11558032035827637,\n",
       "  'token': 5,\n",
       "  'token_str': '!',\n",
       "  'sequence': 'بقينا ف زمن اللي سب الدين مش ف لسانه ده!'},\n",
       " {'score': 0.07560965418815613,\n",
       "  'token': 1936,\n",
       "  'token_str': 'u r l',\n",
       "  'sequence': 'بقينا ف زمن اللي سب الدين مش ف لسانه ده url'},\n",
       " {'score': 0.042377058416604996,\n",
       "  'token': 372,\n",
       "  'token_str': '✋',\n",
       "  'sequence': 'بقينا ف زمن اللي سب الدين مش ف لسانه ده ✋'},\n",
       " {'score': 0.020195644348859787,\n",
       "  'token': 906,\n",
       "  'token_str': '😒',\n",
       "  'sequence': 'بقينا ف زمن اللي سب الدين مش ف لسانه ده 😒'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fill(masked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.', 11.2)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_actual) = getLoss(true_sentence=\"بقينا ف زمن اللي سب الدين مش ف لسانه ده نجم\", masked_sentence= masked_sentence)\n",
    "(predicted, loss_actual) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.', 2.04)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_relevant) = getLoss(true_sentence=f\"بقينا ف زمن اللي سب الدين مش ف لسانه ده {predicted}\", masked_sentence=masked_sentence)\n",
    "(predicted, loss_relevant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.', 16.7)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_not_relevant) = getLoss(true_sentence=\"بقينا ف زمن اللي سب الدين مش ف لسانه ده المكتب\", masked_sentence=masked_sentence)\n",
    "(predicted, loss_not_relevant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.7"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_not_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Relevance 27.86 %\n",
      "Percentage of Not Relevant 72.14 %\n"
     ]
    }
   ],
   "source": [
    "# add penality point to relevant distance because it didn't appeared in the top 100 predicted words \n",
    "point = .5 * loss_actual \n",
    "\n",
    "loss_actual = loss_actual + point\n",
    "loss_not_relevant = loss_not_relevant - point\n",
    "\n",
    "actual_relevant_dist = abs( loss_actual - loss_relevant  )\n",
    "actual_not_relevant_dist = abs( loss_actual - loss_not_relevant )\n",
    "\n",
    "total_distance  = actual_relevant_dist + actual_not_relevant_dist\n",
    "\n",
    "actual_relevant_precent = actual_not_relevant_dist  / total_distance\n",
    "actual_notrelevant_precent = actual_relevant_dist / total_distance\n",
    "\n",
    "print(f\"Percentage of Relevance {round(actual_relevant_precent * 100 , 2)} %\" )\n",
    "print(f\"Percentage of Not Relevant {round(actual_notrelevant_precent * 100 , 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs( loss_actual - loss_not_relevant  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.16"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs( loss_actual - loss_relevant  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.66"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_distancs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.vocab.keys()- pred_top_100.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[UNK]', 'ﻈ', '##ﺮ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"##ﻈﺮ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def getRelevanceScore(true_sentence, word):\n",
    "    masked_sentence = true_sentence.replace(word,\"[MASK]\")\n",
    "    pred_top_100 = getTopPred(masked_sentence)\n",
    "    irrelevant_word = random.choices(list(tokenizer.vocab.keys()- pred_top_100.split()), k=1)[0]\n",
    "    print(\"irrelevant_word in pred_top_100: \",irrelevant_word, irrelevant_word in pred_top_100)\n",
    "    (predicted, loss_actual) = getLoss(true_sentence, masked_sentence)\n",
    "    (predicted, loss_relevant) = getLoss(true_sentence.replace(word, predicted), masked_sentence)\n",
    "    n_tokens = len(tokenizer.tokenize(irrelevant_word))\n",
    "    print(\"n_tokens: \",n_tokens)\n",
    "    (predicted, loss_not_relevant) = getLoss(true_sentence.replace(word, irrelevant_word), masked_sentence = true_sentence.replace(word, \" \".join([\"[MASK]\"]*n_tokens)))\n",
    "    \n",
    "    print(\"predicted\", predicted)\n",
    "    print(\"loss_actual\", loss_actual)\n",
    "    print(\"loss_relevant\", loss_relevant)\n",
    "    print(\"loss_not_relevant\", loss_not_relevant)\n",
    "    \n",
    "    point = .5 * loss_actual \n",
    "    \n",
    "\n",
    "    if word not in pred_top_100:\n",
    "        point = -1* point\n",
    "        print(\"Apply penality  = \", point)\n",
    "    else:\n",
    "        print(\"Apply reward  = \", point)\n",
    "        \n",
    "    loss_actual = loss_actual- point\n",
    "    loss_not_relevant = loss_not_relevant + point\n",
    "    \n",
    "    actual_relevant_dist = abs( loss_actual - loss_relevant  )\n",
    "    actual_not_relevant_dist = abs( loss_actual - loss_not_relevant )\n",
    "\n",
    "    total_distance  = actual_relevant_dist + actual_not_relevant_dist\n",
    "\n",
    "    actual_relevant_precent = actual_not_relevant_dist  / total_distance\n",
    "    actual_notrelevant_precent = actual_relevant_dist / total_distance\n",
    "\n",
    "    print(f\"Percentage of Relevance {round(actual_relevant_precent * 100 , 2)} %\" )\n",
    "    print(f\"Percentage of Not Relevant {round(actual_notrelevant_precent * 100 , 2)} %\")\n",
    "    return (actual_relevant_precent, actual_notrelevant_precent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irrelevant_word in pred_top_100:  مبيبقاش False\n",
      "n_tokens:  1\n",
      "predicted .\n",
      "loss_actual 11.2\n",
      "loss_relevant 2.04\n",
      "loss_not_relevant 16.65\n",
      "Apply penality  =  -5.6\n",
      "Percentage of Relevance 28.04 %\n",
      "Percentage of Not Relevant 71.96 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2803510482691369, 0.719648951730863)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "getRelevanceScore(\"بقينا ف زمن اللي سب الدين مش ف لسانه ده نجم\",\"نجم\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irrelevant_word in pred_top_100:  فاشغله False\n",
      "n_tokens:  1\n",
      "predicted عجل\n",
      "loss_actual 6.78\n",
      "loss_relevant 2.56\n",
      "loss_not_relevant 19.68\n",
      "Apply reward  =  3.39\n",
      "Percentage of Relevance 95.95 %\n",
      "Percentage of Not Relevant 4.05 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9595319356411507, 0.04046806435884935)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRelevanceScore(\"انا رايح اركب المشروع\",\"المشروع\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa01416571b4275a2309b961d1bd16d947c31c3c9a0101a66fce0e66d3b1e2ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
