{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\river\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at ../MARBERT_pytorch_verison/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../MARBERT_pytorch_verison/\",local_files_only=True)\n",
    "model = BertForMaskedLM.from_pretrained(\"../MARBERT_pytorch_verison/\",local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "fill = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoss(true_sentence ,masked_sentence):\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # retrieve index of [MASK]\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "    # print(logits.shape)\n",
    "    predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "    # print(predicted_token_id)\n",
    "    labels = tokenizer(true_sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    # mask labels of non-[MASK] tokens\n",
    "    labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    return (tokenizer.decode(predicted_token_id), round(outputs.loss.item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def getTopPred(masked_sentence,topN = 100):\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "    predicted_token_id = logits[0, mask_token_index]    \n",
    "    topN_tokend_id = np.argpartition(predicted_token_id.reshape(-1), -topN)[-topN:]\n",
    "    return tokenizer.decode(topN_tokend_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ø§Ù„Ù†Ø§Ø³â‰ Ù„Ù† Ø§Ù‚Ø¯Ø±Ø§Ù„Ù„Ù‡ Ø±Ø¨Ù…Ø§ Ø§Ù„Ø¯ÙŠÙ† Ù‡Ù†Ø§ Ø¯Ø§ Ø¨Ø¹Ø¯ÙŠÙ† Ø±Ø¨ÙŠ Ù„ÙƒÙ‰ Ù‡Ù‰ ÙÙŠÙ…Ø§ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙˆØ§Ù„Ù„Ù‡ Ø§Ù„Ø¨Ø§Ø¯ÙŠ Ø±Ø¨Ù‰ Ù†Ø§ Ø§Ù†ØªØ§ Ø§Ù„Ù…ÙˆÙ„Ù‰ ÙØ¹Ù„Ø§ Ø³Ø¨Ø­Ø§Ù†Ù‡ Ù„Ù… Ø¨ÙƒØ±Ù‡ Ø§Ù„Ø°Ù‰ ÙŠØ§Ø±Ø¨ Ø§Ù„Ø¨Ø§Ù‚Ù‰ Ù‡Ùˆ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø¨Ø§Ù„Ù„Ù‡ Ø¯Ù‡ Ù‡Ù… Ø§Ù†ØªÙ… ÙØ§Ù„Ù„Ù‡ Ø§ÙƒÙŠØ¯ Ø±Ø¨ ÙƒØ§Ù† ØªØ¹Ø§Ù„Ù‰ Ø±Ø¨Ù†Ø§ Ø¨Ø³ ÙˆØ­Ø¯Ù‡ Ø±Ø¨ÙƒÙ… Ø§Ù„Ø±Ø¨ Ø§Ù„Ø±Ø­Ù…Ù† Ø±ÙŠÙ†Ø§ Ø§Ù„Ù„Ù‰ ÙˆØ±Ø¨Ù†Ø§ Ø±ÙŠÙŠ Ø§Ù†Ø§ Ø§Ù†Øª Ø§Ù„Ø´Ø§Ø±Ø¹ Ø§Ù„Ø­Ù‚ Ø§Ù„Ù„Û Ø±Ø¨Ù†Ø§Ø§ Ù„ÙƒÙ† Ø§ÙƒØ§Ø¯ Ø§Ù„Ù„Ù‡Ù… Ù…Ø§ Ù„Ù„Ù‡ Ù‡ÙˆØ§ Ø§Ù†Ù‰ Ù†Ø¹Ù… Ø³ÙˆÙ Ø§Ù„Ø±Ø³ÙˆÙ„ Ø§Ù„Ù…ÙˆÙÙ‚ Ù„Ø§ Ø§Ù„Ø´Ø±Ø¹ Ø§Ø­Ù†Ø§ Ø§Ù† Ø§Ù„Ù„Ù‡ Ø¨ÙƒØ±Ø© ØºØ¯Ø§ ï·² Ø§Ù„Ù„Ú¾ url Ø§Ù„Ù„Ù‡Ù‡ Ù‡ÙˆÙ‡ Ø±Ø¨Ùƒ Ø§Ù„Ù‚Ø§Ø¯Ù… Ø§Ø§Ù„Ù„Ù‡ ÙÙ‚Ø· Ù†Ø­Ù† Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ ÙˆØ§Ù„Ù„Ø© ÙƒÙ†Øª Ø§Ù„Ø­Ø§Ø¶Ø± Ù‡Ù…Ø§ Ø§Ù„Ø¹Ø¨Ø§Ø¯ Ø±Ø³ÙˆÙ„Ù‡ Ø§Ù„Ø®Ø§Ù„Ù‚ Ø­Ø¶Ø±ØªÙƒ Ø§Ù„Ù„ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø¹Ø§ÙŠØ² Ù… Ø§Ù„ÙØ¹Ù„ Ø·Ø¨Ø¹Ø§ Ø§Ù†ØªÙ‰ Ø§Ù„Ù„Ø©'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTopPred(\"Ø¯Ø§ Ù…Ø¬Ø±Ø¯ Ø±Ø£Ù‰ Ùˆ[MASK] Ø§Ø¹Ù„Ù… Ùˆ Ø§Ø¨Ù‚Ø§\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 100000])\n",
      "tensor([1944])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Ø§Ù„Ù„Ù‡', 0.11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLoss(true_sentence=\"Ø¯Ø§ Ù…Ø¬Ø±Ø¯ Ø±Ø£Ù‰ Ùˆ Ø§Ù„Ù„Ù‡ Ø§Ø¹Ù„Ù… Ùˆ Ø§Ø¨Ù‚Ø§\", masked_sentence=\"Ø¯Ø§ Ù…Ø¬Ø±Ø¯ Ø±Ø£Ù‰ Ùˆ[MASK] Ø§Ø¹Ù„Ù… Ùˆ Ø§Ø¨Ù‚Ø§\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8920164108276367,\n",
       "  'token': 1944,\n",
       "  'token_str': 'Ø§ Ù„ Ù„ Ù‡',\n",
       "  'sequence': 'Ø¯Ø§ Ù…Ø¬Ø±Ø¯ Ø±Ø§Ù‰ Ùˆ Ø§Ù„Ù„Ù‡ Ø§Ø¹Ù„Ù… Ùˆ Ø§Ø¨Ù‚Ø§'},\n",
       " {'score': 0.05682622268795967,\n",
       "  'token': 2410,\n",
       "  'token_str': 'Ø± Ø¨ Ù† Ø§',\n",
       "  'sequence': 'Ø¯Ø§ Ù…Ø¬Ø±Ø¯ Ø±Ø§Ù‰ Ùˆ Ø±Ø¨Ù†Ø§ Ø§Ø¹Ù„Ù… Ùˆ Ø§Ø¨Ù‚Ø§'},\n",
       " {'score': 0.012048384174704552,\n",
       "  'token': 2043,\n",
       "  'token_str': 'Ø§ Ù† Ø§',\n",
       "  'sequence': 'Ø¯Ø§ Ù…Ø¬Ø±Ø¯ Ø±Ø§Ù‰ Ùˆ Ø§Ù†Ø§ Ø§Ø¹Ù„Ù… Ùˆ Ø§Ø¨Ù‚Ø§'},\n",
       " {'score': 0.008924596942961216,\n",
       "  'token': 2188,\n",
       "  'token_str': 'Ùˆ Ø§ Ù„ Ù„ Ù‡',\n",
       "  'sequence': 'Ø¯Ø§ Ù…Ø¬Ø±Ø¯ Ø±Ø§Ù‰ Ùˆ ÙˆØ§Ù„Ù„Ù‡ Ø§Ø¹Ù„Ù… Ùˆ Ø§Ø¨Ù‚Ø§'},\n",
       " {'score': 0.0060321856290102005,\n",
       "  'token': 16954,\n",
       "  'token_str': 'Ø± Ø¨ Ù‰',\n",
       "  'sequence': 'Ø¯Ø§ Ù…Ø¬Ø±Ø¯ Ø±Ø§Ù‰ Ùˆ Ø±Ø¨Ù‰ Ø§Ø¹Ù„Ù… Ùˆ Ø§Ø¨Ù‚Ø§'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø¯Ø§ Ù…Ø¬Ø±Ø¯ Ø±Ø£Ù‰ Ùˆ Ø§Ù„Ù„Ù‡ Ø§Ø¹Ù„Ù… Ùˆ Ø§Ø¨Ù‚Ø§\n",
    "fill(\"Ø¯Ø§ Ù…Ø¬Ø±Ø¯ Ø±Ø£Ù‰ Ùˆ[MASK] Ø§Ø¹Ù„Ù… Ùˆ Ø§Ø¨Ù‚Ø§\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ù…Ø³Ø§ÙØ±', 6.15)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLoss(true_sentence=\"Ù‡Ùˆ ÙƒÙ„ ÙŠÙˆÙ… Ù†Ø³Ù…Ø¹ ÙØªÙˆÙ‰ Ø¬Ø¯ÙŠØ¯Ù‡  Ù…Ø§ÙƒÙ„Ù†Ø§ Ø¹Ø§Ø±ÙÙŠÙ† Ù…Ù† ÙƒØ§Ù† Ù…Ø±ÙŠØ¶Ø§ Ø£Ùˆ Ø¹Ù„Ù‰ Ø³ÙØ± ÙˆØ®Ù„ØµØª  Ù‡Ù†Ø®ØªØ±Ø¹ Ø¨Ù‚Ù‰ Ø±Ø®Øµ Ø¬Ø¯ÙŠØ¯Ù‡ Ù…Ù† Ø¹Ù†Ø¯Ù†Ø§ \", masked_sentence=\"Ù‡Ùˆ ÙƒÙ„ ÙŠÙˆÙ… Ù†Ø³Ù…Ø¹ ÙØªÙˆÙ‰ Ø¬Ø¯ÙŠØ¯Ù‡  Ù…Ø§ÙƒÙ„Ù†Ø§ Ø¹Ø§Ø±ÙÙŠÙ† Ù…Ù† ÙƒØ§Ù† [MASK] Ø£Ùˆ Ø¹Ù„Ù‰ Ø³ÙØ± ÙˆØ®Ù„ØµØª  Ù‡Ù†Ø®ØªØ±Ø¹ Ø¨Ù‚Ù‰ Ø±Ø®Øµ Ø¬Ø¯ÙŠØ¯Ù‡ Ù…Ù† Ø¹Ù†Ø¯Ù†Ø§ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ø§Ù„Ù…Ø´ØªØ±ÙˆØ¹\" in tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07754268497228622,\n",
       "  'token': 12264,\n",
       "  'token_str': 'Ø¹ Ø¬ Ù„',\n",
       "  'sequence': 'Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ Ø¹Ø¬Ù„'},\n",
       " {'score': 0.052763354033231735,\n",
       "  'token': 33856,\n",
       "  'token_str': 'Øª Ø§ Ùƒ Ø³ ÙŠ',\n",
       "  'sequence': 'Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ ØªØ§ÙƒØ³ÙŠ'},\n",
       " {'score': 0.04821640998125076,\n",
       "  'token': 30165,\n",
       "  'token_str': 'Ø§ Ù„ Ù‚ Ø· Ø±',\n",
       "  'sequence': 'Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ Ø§Ù„Ù‚Ø·Ø±'},\n",
       " {'score': 0.04384848475456238,\n",
       "  'token': 60969,\n",
       "  'token_str': 'Øª Ùˆ Ùƒ Øª Ùˆ Ùƒ',\n",
       "  'sequence': 'Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ ØªÙˆÙƒØªÙˆÙƒ'},\n",
       " {'score': 0.04264984279870987,\n",
       "  'token': 1936,\n",
       "  'token_str': 'u r l',\n",
       "  'sequence': 'Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ url'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ Ø§Ù„Ù…Ø´ØªØ±ÙˆØ¹\n",
    "fill(\"Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ø§ÙˆØªÙˆØ³ØªØ±Ø§Ø¯ Ø§Ù„Ù‡Ø±Ù… ØªØ§Ù†ÙŠ Ù‚Ø§Ø±Ø¨ Ø§Ù„Ø¨ÙˆÙƒÙŠÙ…ÙˆÙ† Ø¬Ù†Ø§Ø²Ø© Ù…ÙˆØ§ØµÙ„Ø© ÙÙŠØ±Ø§Ø±ÙŠ Ù…ÙˆØªÙˆ ØµØ§Ø±ÙˆØ® ÙˆØ§Ø­Ø¯ Ø§Ù„Ø³ÙŠØ³ÙŠ ÙƒÙØ± Ø¯Ù„ÙˆÙ‚ØªÙŠ Ø§Ù„Ù‚Ù…Ø±. Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø§Ù„Ù…Ø§ØªØ´ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ÙŠÙƒØ³ Ø§Ù„ØªÙˆÙƒØªÙˆÙƒ Ù…Ø±Ø§Ø¬ÙŠØ­ ØªØ±Ø§Ù… Ø­Ø¯ÙŠØ¯ Ø·ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¨Ø­Ø± ÙˆØ±Ø§ Ø§Ù„ØµØ­Ø±Ø§ÙˆÙŠ Ø§Ù„Ø³ÙŠØ§Ø±Ù‡ Ø¨Ù†Ø²ÙŠÙ† Ø§Ù„Ù…ÙˆØ¬ ØªÙ‚ÙˆÙŠÙ… Ø¹Ø±Ø¨ÙŠØ© Ø¹Ø±Ø¨ÙŠÙ‡ Ø¨ÙˆÙƒÙŠÙ…ÙˆÙ† Ø§Ù„Ø§Ø³Ø¹Ø§Ù Ø³ÙˆØ§Ù‚ Ø§Ù„Ø²Ù…Ø§Ù„Ùƒ Ù…Ø±ÙƒØ¨ Ø­Ø§ÙÙ„Ø© Ø§Ù„Ø¬ÙŠØ´ Ù„ÙˆØ­Ø¯ÙŠ Ù…ÙˆØ§ØµÙ„Ù‡ Ø§ÙŠÙ‡ Ø³ÙÙŠÙ†Ø© Ø§Ù„Ø´Ù†Ø·Ù‡ Ø§Ù„Ø¯Ø§ÙŠØ±ÙŠ [UNK] Ø­Ø´ÙŠØ´ Ù…ÙƒÙ†Ø© Ø§Ù„Ù…Ø­ÙˆØ± ØªÙƒØ³ÙŠ Ø¨ØªØ±ÙˆÙ„ bmw Ø§Ù„Ø®ÙŠÙ„ Ø§Ù„Ø­ØµØ§Ù† Ø­ØµØ§Ù† Ø§Ù„Ù…Ø±Ø³ÙŠØ¯Ø³ Ø§Ù‡Ùˆ Ø§Ù„ØµØ§Ø±ÙˆØ® Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª Ø§Ù„Ù‡ÙˆØ§ Ø­Ù…Ø§Ø± Ø§Ù„Ø¹Ø¬Ù„Ù‡ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆØ¨Ø§Øµ Ø§ØªÙˆØ¨ÙŠØ³ Ø¹Ø¬Ù„Ø© Ø®ÙŠÙ„ ØªØ±Ø­Ø§Ù„ Ù…ØªØ±Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø·ÙŠØ§Ø±Ù‡ Ù…Ø±Ø³ÙŠØ¯Ø³ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¹Ø¬Ù„ Ù…ÙˆØ§ØµÙ„Ø§Øª Ø§Ù„Ø·ÙŠØ§Ø±Ù‡ url Ø§Ù„Ù‚Ø·Ø± ÙƒØ±ÙŠÙ… Ø§Ù„Ù…ÙˆØ¬Ø© Ø§Ù„Ø¹Ø¬Ù„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‡ Ø·ÙŠØ§Ø±Ø© Ù‚Ø·Ø§Ø± Ù‚Ø·Ø± Ø§Ù„Ø·ÙŠØ§Ø±Ø© ØªÙˆÙƒØªÙˆÙƒ Ø§ÙˆØ¨Ø± Ù…ÙˆØ¬Ø© Ù…ÙŠÙƒØ±ÙˆØ¨Ø§Øµ Ø§Ù„Ù…ØªØ±Ùˆ Ø§Ù„Ø¨Ø§Øµ ØªØ§ÙƒØ³ÙŠ Ø¨Ø§Øµ Ø¬Ù…Ù„ Ø§Ù„Ø§ØªÙˆØ¨ÙŠØ³ Ø§Ù„Ù‚Ø·Ø§Ø± Ø§Ù„ØªØ§ÙƒØ³ÙŠ Ø¹Ø¬Ù„Ù‡'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_top_100 = getTopPred(\"Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ [MASK]\")\n",
    "pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ø§Ù„Ù…Ø´Ø±ÙˆØ¹\" in tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ø§Ù„Ù…Ø´Ø±ÙˆØ¹\" in pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ø§Ù„Ù…ÙƒØªØ¨\" in pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ø¹Ø¬Ù„', 6.78)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_actual) = getLoss(true_sentence=\"Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹\", masked_sentence=\"Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ [MASK]\")\n",
    "(predicted, loss_actual) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ø¹Ø¬Ù„', 2.56)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_relevant) = getLoss(true_sentence=\"Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ Ø¹Ø¬Ù„\", masked_sentence=\"Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ [MASK]\")\n",
    "(predicted, loss_relevant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ø¹Ø¬Ù„', 10.85)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_not_relevant) = getLoss(true_sentence=\"Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ Ø§Ù„Ù…ÙƒØªØ¨\", masked_sentence=\"Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ [MASK]\")\n",
    "(predicted, loss_not_relevant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Relevance 92.89 %\n",
      "Percentage of Not Relevant 7.11 %\n"
     ]
    }
   ],
   "source": [
    "# add award point to relevant distance because it appeared in the top 100 predicted words \n",
    "point = .5 * loss_actual \n",
    "\n",
    "loss_actual = loss_actual- point\n",
    "loss_not_relevant = loss_not_relevant + point\n",
    "\n",
    "actual_relevant_dist = abs( loss_actual - loss_relevant  )\n",
    "actual_not_relevant_dist = abs( loss_actual - loss_not_relevant )\n",
    "\n",
    "total_distance  = actual_relevant_dist + actual_not_relevant_dist\n",
    "\n",
    "actual_relevant_precent = actual_not_relevant_dist  / total_distance\n",
    "actual_notrelevant_precent = actual_relevant_dist / total_distance\n",
    "\n",
    "print(f\"Percentage of Relevance {round(actual_relevant_precent * 100 , 2)} %\" )\n",
    "print(f\"Percentage of Not Relevant {round(actual_notrelevant_precent * 100 , 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_relevant_precent + actual_notrelevant_precent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After correcting \"Ù†Ø±Ù…\" >> \"Ù†Ø¬Ù…\"\n",
    "true_sentence = \"Ø¨Ù‚ÙŠÙ†Ø§ Ù Ø²Ù…Ù† Ø§Ù„Ù„ÙŠ Ø³Ø¨ Ø§Ù„Ø¯ÙŠÙ† Ù…Ø´ Ù Ù„Ø³Ø§Ù†Ù‡ Ø¯Ù‡ Ù†Ø¬Ù…\"\n",
    "masked_sentence = \"Ø¨Ù‚ÙŠÙ†Ø§ Ù Ø²Ù…Ù† Ø§Ù„Ù„ÙŠ Ø³Ø¨ Ø§Ù„Ø¯ÙŠÙ† Ù…Ø´ Ù Ù„Ø³Ø§Ù†Ù‡ Ø¯Ù‡ [MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ù†Ø±Ù…\" in tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ù†Ø¬Ù…\" in tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ù†Ø¬Ù…\" in pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ğŸ˜ğŸ˜ Ø¯Ù„ÙˆÙ‚ØªÙŠ Ø§Ø±Ø­Ù…ÙˆÙ†Ø§ ØªØ§Ù†ÙŠâ™¨ ğŸ’” Ø§Ø®Ù„Ø§Ù‚Ù†Ø§ ğŸ˜‘ğŸ˜‘ - Ø§Ø±Ù‡Ø§Ø¨ Ø¯ÙŠÙ†Ù‡ ğŸ‘ŒğŸ» Ø§Ø¨ØªÙ„Ø§Ø¡ â˜º ØªÙˆÙŠØªØ± ØºÙ„Ø· Ø§Ø®Ù„Ø§Ù‚ Ø¬Ù‡Ù„ ğŸ˜… ğŸ˜¡ ğŸ˜“ ğŸ˜€ Ø®Ù„Ø§Øµ ğŸ˜ Ø¹Ø§Ø¯ÙŠ Ù…Ø¬ØªÙ…Ø¹ğŸŒŸ Ø§Ù„Ù†ÙØ§Ù‚ ğŸ˜  ğŸ˜Šâ˜ Ù…ØµÙŠØ¨Ù‡ ÙƒÙ…Ø§Ù† Ø§Ù†ØªØ´Ø± Ø§Ø³Ù„ÙˆØ¨ ÙƒÙ„Ù‡ Ø³ØªØ§ÙŠÙ„ âœ‹âœ‹ Ù‚Ø±Ù. Ù…ÙˆØ¶Ù‡ Ø§Ø®Ø±Ù‡ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ù…Ø±Ø¶ Ø­Ù‚ÙŠÙ‚Ù‡ Ø¨Ø³ Ø§Ø³Ù…Ù‡ ÙˆØ§Ù„Ù„Ù‡ Ù„Ø¹Ù†Ù‡6 ğŸ‘ âœ‹ ğŸ˜ ğŸ‘Œ Ù‡Ø¨Ù„ ğŸ‘Š Ø®Ø§Ù„Øµ ğŸ˜ Ù†ÙØ§Ù‚ Ù„Ù„Ø§Ø³Ù Ø¨Ù‚ÙŠ Ù‡Ø²Ø§Ø± ğŸ˜‘ Ø­ÙˆØ§Ø± ÙƒÙ„Ø§Ù… ğŸ˜ƒ Ø¨Ø¬Ø¯ Ø­Ù‚ÙŠÙ‚ÙŠ ğŸ˜‚ ØªØ®Ù„Ù ØŸ Ø§Ù„Ø²Ù…Ù† ğŸ˜’ Ø¨Ù‚Ù‰Ù¨ Ø¹ÙŠØ¨ ÙˆØ§Ù‚Ø¹'Ø­Ù„Ø§Ù„ ÙƒØ§ÙØ± ğŸ˜” ÙØ±Ø¹ÙˆÙ† Ø­Ù‚ÙŠÙ‚Ø© ğŸ˜• ğŸ˜‚ğŸ˜‚ Ø­Ø§Ù„Ù†Ø§ Ù…Ø¬ØªÙ…Ø¹Ù†Ø§ [UNK] ØŒ url! Ø­Ø±Ø§Ù… Ø­Ø§Ù„Ù‡ Ø²Ù…Ø§Ù† Ø²Ù…Ù† Ø¨Ù‚Ø§ ÙƒÙØ± âœ‹ğŸ» ğŸ˜’ğŸ˜’\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_top_100 = getTopPred(masked_sentence)\n",
    "pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Ø§Ù„Ù…ÙƒØªØ¨\" in pred_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1302552968263626,\n",
       "  'token': 16,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Ø¨Ù‚ÙŠÙ†Ø§ Ù Ø²Ù…Ù† Ø§Ù„Ù„ÙŠ Ø³Ø¨ Ø§Ù„Ø¯ÙŠÙ† Ù…Ø´ Ù Ù„Ø³Ø§Ù†Ù‡ Ø¯Ù‡.'},\n",
       " {'score': 0.11558032035827637,\n",
       "  'token': 5,\n",
       "  'token_str': '!',\n",
       "  'sequence': 'Ø¨Ù‚ÙŠÙ†Ø§ Ù Ø²Ù…Ù† Ø§Ù„Ù„ÙŠ Ø³Ø¨ Ø§Ù„Ø¯ÙŠÙ† Ù…Ø´ Ù Ù„Ø³Ø§Ù†Ù‡ Ø¯Ù‡!'},\n",
       " {'score': 0.07560965418815613,\n",
       "  'token': 1936,\n",
       "  'token_str': 'u r l',\n",
       "  'sequence': 'Ø¨Ù‚ÙŠÙ†Ø§ Ù Ø²Ù…Ù† Ø§Ù„Ù„ÙŠ Ø³Ø¨ Ø§Ù„Ø¯ÙŠÙ† Ù…Ø´ Ù Ù„Ø³Ø§Ù†Ù‡ Ø¯Ù‡ url'},\n",
       " {'score': 0.042377058416604996,\n",
       "  'token': 372,\n",
       "  'token_str': 'âœ‹',\n",
       "  'sequence': 'Ø¨Ù‚ÙŠÙ†Ø§ Ù Ø²Ù…Ù† Ø§Ù„Ù„ÙŠ Ø³Ø¨ Ø§Ù„Ø¯ÙŠÙ† Ù…Ø´ Ù Ù„Ø³Ø§Ù†Ù‡ Ø¯Ù‡ âœ‹'},\n",
       " {'score': 0.020195644348859787,\n",
       "  'token': 906,\n",
       "  'token_str': 'ğŸ˜’',\n",
       "  'sequence': 'Ø¨Ù‚ÙŠÙ†Ø§ Ù Ø²Ù…Ù† Ø§Ù„Ù„ÙŠ Ø³Ø¨ Ø§Ù„Ø¯ÙŠÙ† Ù…Ø´ Ù Ù„Ø³Ø§Ù†Ù‡ Ø¯Ù‡ ğŸ˜’'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fill(masked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.', 11.2)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_actual) = getLoss(true_sentence=\"Ø¨Ù‚ÙŠÙ†Ø§ Ù Ø²Ù…Ù† Ø§Ù„Ù„ÙŠ Ø³Ø¨ Ø§Ù„Ø¯ÙŠÙ† Ù…Ø´ Ù Ù„Ø³Ø§Ù†Ù‡ Ø¯Ù‡ Ù†Ø¬Ù…\", masked_sentence= masked_sentence)\n",
    "(predicted, loss_actual) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.', 2.04)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_relevant) = getLoss(true_sentence=f\"Ø¨Ù‚ÙŠÙ†Ø§ Ù Ø²Ù…Ù† Ø§Ù„Ù„ÙŠ Ø³Ø¨ Ø§Ù„Ø¯ÙŠÙ† Ù…Ø´ Ù Ù„Ø³Ø§Ù†Ù‡ Ø¯Ù‡ {predicted}\", masked_sentence=masked_sentence)\n",
    "(predicted, loss_relevant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.', 16.7)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted, loss_not_relevant) = getLoss(true_sentence=\"Ø¨Ù‚ÙŠÙ†Ø§ Ù Ø²Ù…Ù† Ø§Ù„Ù„ÙŠ Ø³Ø¨ Ø§Ù„Ø¯ÙŠÙ† Ù…Ø´ Ù Ù„Ø³Ø§Ù†Ù‡ Ø¯Ù‡ Ø§Ù„Ù…ÙƒØªØ¨\", masked_sentence=masked_sentence)\n",
    "(predicted, loss_not_relevant) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.7"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_not_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Relevance 27.86 %\n",
      "Percentage of Not Relevant 72.14 %\n"
     ]
    }
   ],
   "source": [
    "# add penality point to relevant distance because it didn't appeared in the top 100 predicted words \n",
    "point = .5 * loss_actual \n",
    "\n",
    "loss_actual = loss_actual + point\n",
    "loss_not_relevant = loss_not_relevant - point\n",
    "\n",
    "actual_relevant_dist = abs( loss_actual - loss_relevant  )\n",
    "actual_not_relevant_dist = abs( loss_actual - loss_not_relevant )\n",
    "\n",
    "total_distance  = actual_relevant_dist + actual_not_relevant_dist\n",
    "\n",
    "actual_relevant_precent = actual_not_relevant_dist  / total_distance\n",
    "actual_notrelevant_precent = actual_relevant_dist / total_distance\n",
    "\n",
    "print(f\"Percentage of Relevance {round(actual_relevant_precent * 100 , 2)} %\" )\n",
    "print(f\"Percentage of Not Relevant {round(actual_notrelevant_precent * 100 , 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs( loss_actual - loss_not_relevant  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.16"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs( loss_actual - loss_relevant  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.66"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_distancs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.vocab.keys()- pred_top_100.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[UNK]', 'ï»ˆ', '##ïº®']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"##ï»ˆïº®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def getRelevanceScore(true_sentence, word):\n",
    "    masked_sentence = true_sentence.replace(word,\"[MASK]\")\n",
    "    pred_top_100 = getTopPred(masked_sentence)\n",
    "    irrelevant_word = random.choices(list(tokenizer.vocab.keys()- pred_top_100.split()), k=1)[0]\n",
    "    print(\"irrelevant_word in pred_top_100: \",irrelevant_word, irrelevant_word in pred_top_100)\n",
    "    (predicted, loss_actual) = getLoss(true_sentence, masked_sentence)\n",
    "    (predicted, loss_relevant) = getLoss(true_sentence.replace(word, predicted), masked_sentence)\n",
    "    n_tokens = len(tokenizer.tokenize(irrelevant_word))\n",
    "    print(\"n_tokens: \",n_tokens)\n",
    "    (predicted, loss_not_relevant) = getLoss(true_sentence.replace(word, irrelevant_word), masked_sentence = true_sentence.replace(word, \" \".join([\"[MASK]\"]*n_tokens)))\n",
    "    \n",
    "    print(\"predicted\", predicted)\n",
    "    print(\"loss_actual\", loss_actual)\n",
    "    print(\"loss_relevant\", loss_relevant)\n",
    "    print(\"loss_not_relevant\", loss_not_relevant)\n",
    "    \n",
    "    point = .5 * loss_actual \n",
    "    \n",
    "\n",
    "    if word not in pred_top_100:\n",
    "        point = -1* point\n",
    "        print(\"Apply penality  = \", point)\n",
    "    else:\n",
    "        print(\"Apply reward  = \", point)\n",
    "        \n",
    "    loss_actual = loss_actual- point\n",
    "    loss_not_relevant = loss_not_relevant + point\n",
    "    \n",
    "    actual_relevant_dist = abs( loss_actual - loss_relevant  )\n",
    "    actual_not_relevant_dist = abs( loss_actual - loss_not_relevant )\n",
    "\n",
    "    total_distance  = actual_relevant_dist + actual_not_relevant_dist\n",
    "\n",
    "    actual_relevant_precent = actual_not_relevant_dist  / total_distance\n",
    "    actual_notrelevant_precent = actual_relevant_dist / total_distance\n",
    "\n",
    "    print(f\"Percentage of Relevance {round(actual_relevant_precent * 100 , 2)} %\" )\n",
    "    print(f\"Percentage of Not Relevant {round(actual_notrelevant_precent * 100 , 2)} %\")\n",
    "    return (actual_relevant_precent, actual_notrelevant_precent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irrelevant_word in pred_top_100:  Ù…Ø¨ÙŠØ¨Ù‚Ø§Ø´ False\n",
      "n_tokens:  1\n",
      "predicted .\n",
      "loss_actual 11.2\n",
      "loss_relevant 2.04\n",
      "loss_not_relevant 16.65\n",
      "Apply penality  =  -5.6\n",
      "Percentage of Relevance 28.04 %\n",
      "Percentage of Not Relevant 71.96 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2803510482691369, 0.719648951730863)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "getRelevanceScore(\"Ø¨Ù‚ÙŠÙ†Ø§ Ù Ø²Ù…Ù† Ø§Ù„Ù„ÙŠ Ø³Ø¨ Ø§Ù„Ø¯ÙŠÙ† Ù…Ø´ Ù Ù„Ø³Ø§Ù†Ù‡ Ø¯Ù‡ Ù†Ø¬Ù…\",\"Ù†Ø¬Ù…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irrelevant_word in pred_top_100:  ÙØ§Ø´ØºÙ„Ù‡ False\n",
      "n_tokens:  1\n",
      "predicted Ø¹Ø¬Ù„\n",
      "loss_actual 6.78\n",
      "loss_relevant 2.56\n",
      "loss_not_relevant 19.68\n",
      "Apply reward  =  3.39\n",
      "Percentage of Relevance 95.95 %\n",
      "Percentage of Not Relevant 4.05 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9595319356411507, 0.04046806435884935)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRelevanceScore(\"Ø§Ù†Ø§ Ø±Ø§ÙŠØ­ Ø§Ø±ÙƒØ¨ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹\",\"Ø§Ù„Ù…Ø´Ø±ÙˆØ¹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa01416571b4275a2309b961d1bd16d947c31c3c9a0101a66fce0e66d3b1e2ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
