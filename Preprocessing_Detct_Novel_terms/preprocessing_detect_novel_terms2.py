# -*- coding: utf-8 -*-
"""Preprocessing_Detect_Novel_Terms2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1idZaRlAdz_bfsnE61GnC_d53bxBu_Ha5

#**Reading the data**
"""

# !pip install ar-corrector

import re
import pandas as pd
# from collections import Counter
# import nltk
# from nltk.corpus import wordnet
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer
# from nltk.stem.porter import PorterStemmer
import numpy as np
# from nltk.corpus import stopwords
# import os
# import warnings
import itertools
# import emoji
from nltk.corpus import stopwords
import string
# from ar_corrector.corrector import Corrector

# data = pd.read_excel("/content/train_data .xlsx")

# teeext=data.iloc[11,0]
# teeext

# data

# data['tweet'].describe()

# data['Source'].unique()

"""#**Preproccessing**

#### Data Cleaning & Normlization
>*  Remove Punctuatuions
*  Remove Usernames(mentions)
*   Remove URLs
*   Remove emoji
*   Remove Numbers
*   Remove Special Characters
*   Remove Elongation
"""

# !pip install farasapy

# !git clone https://github.com/aub-mind/arabert

# !pip install pyarabic

# !pip install pyspellchecker

def removeshortsen(txt):
  words=txt.split(' ')
  text=''
  if len(words) > 4:
    text = ' '.join(words)

  return text

def removeMention(txt):
  arabic_men=re.sub("@[أ-ي]+","",txt)#//remove Arabic hashtags that have  after it
  arabic_men=re.sub("[أ-ي]@+","",arabic_men) #remove Arabic hashtags that have # before it
  without_men=re.sub("@[A-Za-z]+","",arabic_men)# remove English hashtags
  return without_men

def removeduplicate(txt):
  non_duplicate = ''.join(i for i, _ in itertools.groupby(txt))
  return non_duplicate

def reduce_characters(inputText):
    '''
    step #4: Reduce character repitation of > 2 characters at time
              For example: the word 'cooooool' will convert to 'cool'
    '''
    # pattern to look for three or more repetitions of any character, including
    # newlines.
    pattern = re.compile(r"(.)\1{2,}", re.DOTALL)
    reduced_text = pattern.sub(r"\1\1", inputText)
    return reduced_text

def removeDigits(txt):
  without_digit = re.sub('[0-9]+', '', txt) 
  return without_digit

def removeURl(txt):
  without_url= re.sub('((www\.[^\s]+)|(https?://[^\s]+))', ' ', txt)
  return without_url

def removestopwords(txt):
  without_stopwords = ' '.join(word for word in txt.split() if word not in stopwords)
  return without_stopwords

def removepuncatution(txt):

  punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation
  translator = str.maketrans('', '', punctuations)
  without_punc = txt.translate(translator)

  # txtt = re.sub("[\s+\\# ـ!\/_,$%=^*?:@&^~`(+\"]+|[+！，。？、~@￥%……&*（）“ ”:;：；、\\《）《》“”()»〔〕# ]+ ·.『]", "", txt)
  # seq = re.sub("[\s+[`÷×؛<>_()*&% ـ،/:{} ÷×؛<>_()*&^%][ـ،/: ]", "", txtt)
  return without_punc

def removenonrabic(txt):
  # Remove None arabic 
  without_english = re.sub(u"[^\u0621-\u063A\u0640-\u0652 ]", " ", txt)
  return without_english

def removeHashtages(txt):
  arabic_hash=re.sub("#[أ-ي]+","",txt)#//remove Arabic hashtags that have  after it
  arabic_hash_=re.sub("[أ-ي]#+","",arabic_hash) #remove Arabic hashtags that have # before it
  without_hash=re.sub("#[A-Za-z]+","",arabic_hash)# remove English hashtags
  return without_hash

def removetashkeel(txt):
  TATWEEL = u"\u0640"
  p_tashkeel = re.compile(r'[\u0617-\u061A\u064B-\u0652]')
  without_tashkeel = re.sub(p_tashkeel,"", txt)
  without_tashkeel=without_tashkeel.replace(TATWEEL, '')
  return without_tashkeel

def remove_Emojis(text):
  emoj = re.compile("["
      u"\U0001F600-\U0001F64F"  # emoticons
      u"\U0001F300-\U0001F5FF"  # symbols & pictographs
      u"\U0001F680-\U0001F6FF"  # transport & map symbols
      u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
      u"\U00002500-\U00002BEF"  # chinese char
      u"\U00002702-\U000027B0"
      u"\U00002702-\U000027B0"
      u"\U000024C2-\U0001F251"
      u"\U0001f926-\U0001f937"
      u"\U00010000-\U0010ffff"
      u"\u2640-\u2642" 
      u"\u2600-\u2B55"
      u"\u200d"
      u"\u23cf"
      u"\u23e9"
      u"\u231a"
      u"\ufe0f"  # dingbats
      u"\u3030"
                    "]+", re.UNICODE)
  without_emojy=re.sub(emoj, '', text)
  return without_emojy

# corr = Corrector()
# def correction(text):
#   text=corr.contextual_correct(text)
#   return text

def Clean_Egyptain_text(raw_text):

  raw_text=raw_text.strip()

  #1 remove shorten sentences that contain less than 4 words 
  # text=removeshortsen(raw_text)

  #2 remove duplichated chars in the raw text
  #text=removeduplicate(text)
  text=reduce_characters(raw_text)

  #3 remove hashtages after remove duplicated chars
  text=removeHashtages(text)

  #4 remove digits
  text= removeDigits(text)

  #5 remove the puncatution from the text 
  text=removepuncatution(text)

  #6 remove the mention and @ char
  text=removeMention(text)

  #7 remove arabic tashkeel from the text
  text=removetashkeel(text)

  #8 remove Emojies from the text
  text=remove_Emojis(text)

  #9 remove url and links from the text
  text=removeURl(text)

  # #9 remove stopwords from the text
  # text=removestopwords(text)

  #10 remove non rabic words from the text
  text=removenonrabic(text)


 #10 remove non rabic words from the text
  # text=correction(text)

  return text

# clean = {
#     'Text': data['tweet'].apply(lambda x: Clean_Egyptain_text(str(x)))
#     }
# Cleaned_data = pd.DataFrame.from_dict(clean)

# filter = Cleaned_data['Text'] != ""
# Cleaned_data = Cleaned_data[filter]

# Cleaned_data

# """#### Check missing data"""

# Cleaned_data.isnull().sum()

# """#### Cleaned data CSV"""

# Cleaned_data.to_csv('clean_dataset2.csv',encoding="utf8")

# """####Get the Vocab from thr trained data"""

# x = Cleaned_data.to_string(header=False,
#                   index=False,
#                   index_names=False).split('\n')
# words = [','.join(ele.split()) for ele in x]
# print(words)

# All_words = [word for item in x for word in item.split()]
# All_words

# type(All_words)

# # file.write("\n".join(names)

# with open("AOC.txt", "w") as output:
#     output.write(str(All_words))

# def split_sentence(text):
#   text = text.strip()
#   tokens = text.split()
#   for i, token in enumerate(tokens):
#           if token.startswith('و'):
#                   tokens[i] = token[0] + ' ' + token[1:]
#           if token.startswith('ولا'):
#                   tokens[i] = token[0] + ' ' +token[1:3] +' '+ token[3:]

#   text = ' '.join(tokens)
#   return text

